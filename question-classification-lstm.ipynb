{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14821512,"sourceType":"datasetVersion","datasetId":9478698}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-13T09:59:42.299359Z","iopub.execute_input":"2026-02-13T09:59:42.300301Z","iopub.status.idle":"2026-02-13T09:59:42.706013Z","shell.execute_reply.started":"2026-02-13T09:59:42.300265Z","shell.execute_reply":"2026-02-13T09:59:42.704803Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/datasets/aabdollahii/university-questions/train.json\n/kaggle/input/datasets/aabdollahii/university-questions/test.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n#  FULL PIPELINE: LSTM for Persian Question Ambiguity Detection\n# ============================================================\n#  Stage 1: Preprocessing V2 (train + test)\n#  Stage 2: Vocabulary & Dataset\n#  Stage 3: LSTM Model Definition\n#  Stage 4: Training with dev (train-as-dev) monitoring\n#  Stage 5: Final test evaluation & model saving\n# ============================================================\n\nimport json\nimport re\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix,\n    f1_score, accuracy_score\n)\n\nfrom hazm import Normalizer, word_tokenize\n\n# ============================================================\n#  CONFIG\n# ============================================================\nclass Config:\n    # Paths\n    TRAIN_PATH = \"/kaggle/input/datasets/aabdollahii/university-questions/train.json\"\n    TEST_PATH = \"/kaggle/input/datasets/aabdollahii/university-questions/test.json\"\n    SAVE_DIR = \"/kaggle/working/\"\n    \n    # Preprocessing\n    MAX_LEN = 64           # max tokens per question (will verify from data)\n    MIN_FREQ = 2           # min word frequency to include in vocab\n    \n    # Model\n    EMBED_DIM = 128\n    HIDDEN_DIM = 128\n    NUM_LAYERS = 2\n    DROPOUT = 0.3\n    BIDIRECTIONAL = True\n    \n    # Training\n    BATCH_SIZE = 32\n    EPOCHS = 10\n    LR = 1e-3\n    WEIGHT_DECAY = 1e-5\n    PATIENCE = 7           # early stopping patience\n    \n    # Device\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Reproducibility\n    SEED = 42\n\ncfg = Config()\n\n# Set seeds\ntorch.manual_seed(cfg.SEED)\nnp.random.seed(cfg.SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(cfg.SEED)\n\nprint(f\"Device: {cfg.DEVICE}\")\nprint(f\"PyTorch version: {torch.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T10:28:49.039025Z","iopub.execute_input":"2026-02-13T10:28:49.039448Z","iopub.status.idle":"2026-02-13T10:28:49.052050Z","shell.execute_reply.started":"2026-02-13T10:28:49.039416Z","shell.execute_reply":"2026-02-13T10:28:49.050979Z"}},"outputs":[{"name":"stdout","text":"Device: cpu\nPyTorch version: 2.8.0+cu126\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================\n#  STAGE 1: PREPROCESSING V2\n# ============================================================\nprint(\"\\n\" + \"=\" * 65)\nprint(\"  STAGE 1: PREPROCESSING V2\")\nprint(\"=\" * 65)\n\nformal_normalizer = Normalizer()\n\ndef normalize_v2(text):\n    \"\"\"\n    V2 normalization pipeline — safe version (no InformalNormalizer).\n    1. Hazm formal normalization (handles ی/ک, spacing, etc.)\n    2. Arabic char normalization\n    3. Clean punctuation/extra whitespace\n    \"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return \"\"\n    \n    # Step 1: Hazm formal normalization\n    text = formal_normalizer.normalize(text)\n    \n    # Step 2: Additional Arabic → Persian char normalization\n    text = text.replace(\"ي\", \"ی\").replace(\"ك\", \"ک\")\n    text = text.replace(\"ؤ\", \"و\").replace(\"إ\", \"ا\").replace(\"أ\", \"ا\")\n    text = text.replace(\"ة\", \"ه\")\n    \n    # Step 3: Normalize various dashes and special chars\n    text = re.sub(r'[ـ]+', '', text)              # remove kashida (tatweel)\n    text = re.sub(r'[‌]+', ' ', text)              # replace ZWNJ with space (hazm handles most)\n    \n    # Step 4: Keep Persian/Arabic letters, digits, basic punctuation, spaces\n    text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\uFB50-\\uFDFF\\uFE70-\\uFEFF'\n                  r'a-zA-Z0-9۰-۹٠-٩\\s\\.\\?\\!،؛]', ' ', text)\n    \n    # Step 5: Clean extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n\ndef tokenize_text(text):\n    \"\"\"Tokenize using Hazm word_tokenize after normalization.\"\"\"\n    if not text:\n        return []\n    return word_tokenize(text)\n\n\n# --- Load Data ---\nprint(\"Loading train.json ...\")\nwith open(cfg.TRAIN_PATH, \"r\", encoding=\"utf-8\") as f:\n    train_data = json.load(f)\ndf_train = pd.DataFrame(train_data)\nprint(f\"  Train shape: {df_train.shape}\")\nprint(f\"  Label distribution:\\n{df_train['is_ambiguous'].value_counts().to_string()}\")\n\nprint(\"\\nLoading test.json ...\")\nwith open(cfg.TEST_PATH, \"r\", encoding=\"utf-8\") as f:\n    test_data = json.load(f)\ndf_test = pd.DataFrame(test_data)\nprint(f\"  Test shape: {df_test.shape}\")\nhas_test_labels = \"is_ambiguous\" in df_test.columns\nif has_test_labels:\n    print(f\"  Test label distribution:\\n{df_test['is_ambiguous'].value_counts().to_string()}\")\n\n# --- Apply Normalization ---\nprint(\"\\nNormalizing train questions ...\")\ndf_train[\"norm_text\"] = df_train[\"question\"].apply(normalize_v2)\nprint(\"Normalizing test questions ...\")\ndf_test[\"norm_text\"] = df_test[\"question\"].apply(normalize_v2)\n\n# --- Tokenize ---\nprint(\"Tokenizing train ...\")\ndf_train[\"tokens\"] = df_train[\"norm_text\"].apply(tokenize_text)\nprint(\"Tokenizing test ...\")\ndf_test[\"tokens\"] = df_test[\"norm_text\"].apply(tokenize_text)\n\n# --- Show Samples ---\nprint(\"\\n--- Train Samples ---\")\nfor i in range(5):\n    print(f\"  [{df_train['is_ambiguous'].iloc[i]}] {df_train['question'].iloc[i]}\")\n    print(f\"       → {df_train['tokens'].iloc[i][:15]} ...\")\n    print()\n\n# --- Sequence Length Analysis ---\ntrain_lengths = df_train[\"tokens\"].apply(len)\nprint(f\"Token length stats (train):\")\nprint(f\"  Mean:   {train_lengths.mean():.1f}\")\nprint(f\"  Median: {train_lengths.median():.1f}\")\nprint(f\"  95th %: {train_lengths.quantile(0.95):.0f}\")\nprint(f\"  99th %: {train_lengths.quantile(0.99):.0f}\")\nprint(f\"  Max:    {train_lengths.max()}\")\n\n# Update MAX_LEN based on data (cover 95th percentile)\nsuggested_max_len = int(train_lengths.quantile(0.95)) + 2\nif suggested_max_len != cfg.MAX_LEN:\n    print(f\"\\n   Updating MAX_LEN: {cfg.MAX_LEN} → {suggested_max_len}\")\n    cfg.MAX_LEN = suggested_max_len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T10:07:26.129209Z","iopub.execute_input":"2026-02-13T10:07:26.129555Z","iopub.status.idle":"2026-02-13T10:19:35.154202Z","shell.execute_reply.started":"2026-02-13T10:07:26.129522Z","shell.execute_reply":"2026-02-13T10:19:35.152813Z"}},"outputs":[{"name":"stdout","text":"\n=================================================================\n  STAGE 1: PREPROCESSING V2\n=================================================================\nLoading train.json ...\n  Train shape: (900, 4)\n  Label distribution:\nis_ambiguous\n0    450\n1    450\n\nLoading test.json ...\n  Test shape: (100, 4)\n  Test label distribution:\nis_ambiguous\n0    50\n1    50\n\nNormalizing train questions ...\nNormalizing test questions ...\nTokenizing train ...\nTokenizing test ...\n\n--- Train Samples ---\n  [0] حداقل نمره قبولی در هر درس برای دانشجویان مقطع کارشناسی چند است؟\n       → ['حداقل', 'نمره', 'قبولی', 'در', 'هر', 'درس', 'برای', 'دانشجویان', 'مقطع', 'کارشناسی', 'چند', 'است', '؟'] ...\n\n  [0] حداکثر سنوات مجاز تحصیل در دوره کارشناسی پیوسته چند نیمسال است؟\n       → ['حداکثر', 'سنوات', 'مجاز', 'تحصیل', 'در', 'دوره', 'کارشناسی', 'پیوسته', 'چند', 'نیمسال', 'است', '؟'] ...\n\n  [0] آیا دانشجوی کارشناسی می‌تواند با معدل بالای ۱۷، بیش از ۲۰ واحد در ترم بعد اخذ کند؟\n       → ['آیا', 'دانشجوی', 'کارشناسی', 'می', 'تواند', 'با', 'معدل', 'بالای', '۱۷', '،', 'بیش', 'از', '۲۰', 'واحد', 'در'] ...\n\n  [0] دانشجوی کارشناسی ارشد حداکثر چند نیمسال می‌تواند از مرخصی تحصیلی استفاده کند؟\n       → ['دانشجوی', 'کارشناسی', 'ارشد', 'حداکثر', 'چند', 'نیمسال', 'می', 'تواند', 'از', 'مرخصی', 'تحصیلی', 'استفاده', 'کند', '؟'] ...\n\n  [0] آخرین مهلت برای حذف تک‌درس (حذف اضطراری) در طول نیمسال چه زمانی است؟\n       → ['آخرین', 'مهلت', 'برای', 'حذف', 'تک', 'درس', 'حذف', 'اضطراری', 'در', 'طول', 'نیمسال', 'چه', 'زمانی', 'است', '؟'] ...\n\nToken length stats (train):\n  Mean:   10.2\n  Median: 10.0\n  95th %: 17\n  99th %: 22\n  Max:    25\n\n   Updating MAX_LEN: 64 → 19\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================\n#  STAGE 2: VOCABULARY & DATASET\n# ============================================================\nprint(\"\\n\" + \"=\" * 65)\nprint(\"  STAGE 2: VOCABULARY & DATASET\")\nprint(\"=\" * 65)\n\n# --- Build Vocabulary (from TRAINING data only) ---\nPAD_TOKEN = \"<PAD>\"\nUNK_TOKEN = \"<UNK>\"\nPAD_IDX = 0\nUNK_IDX = 1\n\nword_counts = Counter()\nfor tokens in df_train[\"tokens\"]:\n    word_counts.update(tokens)\n\nprint(f\"Total unique words in train: {len(word_counts)}\")\n\n# Filter by min frequency\nvocab_words = [w for w, c in word_counts.items() if c >= cfg.MIN_FREQ]\nvocab_words.sort()  # deterministic order\n\nword2idx = {PAD_TOKEN: PAD_IDX, UNK_TOKEN: UNK_IDX}\nfor w in vocab_words:\n    word2idx[w] = len(word2idx)\n\nidx2word = {v: k for k, v in word2idx.items()}\nvocab_size = len(word2idx)\n\nprint(f\"Vocab size (min_freq={cfg.MIN_FREQ}): {vocab_size}\")\nprint(f\"  (including <PAD> and <UNK>)\")\n\n# Check OOV rate on test\nif len(df_test) > 0:\n    test_tokens_all = [t for tokens in df_test[\"tokens\"] for t in tokens]\n    oov_count = sum(1 for t in test_tokens_all if t not in word2idx)\n    print(f\"Test OOV rate: {oov_count}/{len(test_tokens_all)} = \"\n          f\"{oov_count/max(len(test_tokens_all),1)*100:.1f}%\")\n\n\ndef encode_tokens(tokens, word2idx, max_len):\n    \"\"\"Convert token list to padded index array.\"\"\"\n    ids = [word2idx.get(t, UNK_IDX) for t in tokens[:max_len]]\n    length = len(ids)\n    # Pad\n    ids = ids + [PAD_IDX] * (max_len - length)\n    return ids, length\n\n\n# --- PyTorch Dataset ---\nclass QuestionDataset(Dataset):\n    def __init__(self, tokens_list, labels, word2idx, max_len):\n        self.tokens_list = tokens_list\n        self.labels = labels\n        self.word2idx = word2idx\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.tokens_list)\n    \n    def __getitem__(self, idx):\n        tokens = self.tokens_list[idx]\n        label = self.labels[idx]\n        \n        ids, length = encode_tokens(tokens, self.word2idx, self.max_len)\n        \n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"length\": torch.tensor(length, dtype=torch.long),\n            \"label\": torch.tensor(label, dtype=torch.float)\n        }\n\n\n# --- Prepare splits ---\n# Full training set\ntrain_tokens = df_train[\"tokens\"].tolist()\ntrain_labels = df_train[\"is_ambiguous\"].values.astype(int)\n\n# Test set\ntest_tokens = df_test[\"tokens\"].tolist()\nif has_test_labels:\n    test_labels = df_test[\"is_ambiguous\"].values.astype(int)\nelse:\n    test_labels = np.zeros(len(df_test), dtype=int)  # placeholder\n\n# Dev set = train set (as per your request, for debugging)\ndev_tokens = train_tokens\ndev_labels = train_labels\n\n# Create datasets\ntrain_dataset = QuestionDataset(train_tokens, train_labels, word2idx, cfg.MAX_LEN)\ndev_dataset   = QuestionDataset(dev_tokens, dev_labels, word2idx, cfg.MAX_LEN)\ntest_dataset  = QuestionDataset(test_tokens, test_labels, word2idx, cfg.MAX_LEN)\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True)\ndev_loader   = DataLoader(dev_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False)\n\nprint(f\"\\nDataLoaders ready:\")\nprint(f\"  Train: {len(train_dataset)} samples, {len(train_loader)} batches\")\nprint(f\"  Dev:   {len(dev_dataset)} samples (= train, for debugging)\")\nprint(f\"  Test:  {len(test_dataset)} samples\")\n\n# --- Compute class weights ---\nn_pos = train_labels.sum()\nn_neg = len(train_labels) - n_pos\npos_weight = torch.tensor([n_neg / max(n_pos, 1)], dtype=torch.float).to(cfg.DEVICE)\nprint(f\"\\nClass balance: neg={n_neg}, pos={n_pos}\")\nprint(f\"  pos_weight for BCEWithLogitsLoss: {pos_weight.item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T10:25:44.653309Z","iopub.execute_input":"2026-02-13T10:25:44.653686Z","iopub.status.idle":"2026-02-13T10:25:44.704198Z","shell.execute_reply.started":"2026-02-13T10:25:44.653652Z","shell.execute_reply":"2026-02-13T10:25:44.702859Z"}},"outputs":[{"name":"stdout","text":"\n=================================================================\n  STAGE 2: VOCABULARY & DATASET\n=================================================================\nTotal unique words in train: 1155\nVocab size (min_freq=2): 652\n  (including <PAD> and <UNK>)\nTest OOV rate: 90/1026 = 8.8%\n\nDataLoaders ready:\n  Train: 900 samples, 29 batches\n  Dev:   900 samples (= train, for debugging)\n  Test:  100 samples\n\nClass balance: neg=450, pos=450\n  pos_weight for BCEWithLogitsLoss: 1.0000\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================\n#  STAGE 3: LSTM MODEL\n# ============================================================\nprint(\"\\n\" + \"=\" * 65)\nprint(\"  STAGE 3: LSTM MODEL DEFINITION\")\nprint(\"=\" * 65)\n\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers,\n                 dropout, bidirectional, pad_idx):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(\n            vocab_size, embed_dim, padding_idx=pad_idx\n        )\n        \n        self.lstm = nn.LSTM(\n            input_size=embed_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0.0,\n            bidirectional=bidirectional\n        )\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        # If bidirectional, hidden output is 2 * hidden_dim\n        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n        \n        # Attention-like pooling: combine last hidden state + max pool + mean pool\n        self.fc = nn.Sequential(\n            nn.Linear(lstm_output_dim * 3, lstm_output_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(lstm_output_dim, 1)\n        )\n    \n    def forward(self, input_ids, lengths):\n        # input_ids: (batch, max_len)\n        # lengths:   (batch,)\n        \n        embedded = self.dropout(self.embedding(input_ids))\n        # embedded: (batch, max_len, embed_dim)\n        \n        # Pack padded sequences for efficient LSTM\n        packed = nn.utils.rnn.pack_padded_sequence(\n            embedded, lengths.cpu().clamp(min=1),\n            batch_first=True, enforce_sorted=False\n        )\n        \n        lstm_out, (hidden, cell) = self.lstm(packed)\n        \n        # Unpack\n        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(\n            lstm_out, batch_first=True, total_length=input_ids.size(1)\n        )\n        # lstm_out: (batch, max_len, lstm_output_dim)\n        \n        # Create mask for padded positions\n        mask = (input_ids != PAD_IDX).unsqueeze(-1).float()\n        # mask: (batch, max_len, 1)\n        \n        # --- Pooling strategies ---\n        # 1. Last hidden state (concat forward + backward for bidirectional)\n        if self.lstm.bidirectional:\n            # hidden: (num_layers * 2, batch, hidden_dim)\n            last_hidden = torch.cat(\n                [hidden[-2], hidden[-1]], dim=-1\n            )  # (batch, hidden_dim * 2)\n        else:\n            last_hidden = hidden[-1]  # (batch, hidden_dim)\n        \n        # 2. Max pooling (masked)\n        lstm_out_masked = lstm_out * mask + (1 - mask) * (-1e9)\n        max_pool, _ = lstm_out_masked.max(dim=1)  # (batch, lstm_output_dim)\n        \n        # 3. Mean pooling (masked)\n        sum_pool = (lstm_out * mask).sum(dim=1)  # (batch, lstm_output_dim)\n        lengths_expanded = lengths.unsqueeze(-1).float().clamp(min=1).to(sum_pool.device)\n        mean_pool = sum_pool / lengths_expanded\n        \n        # Concatenate all three\n        combined = torch.cat([last_hidden, max_pool, mean_pool], dim=-1)\n        \n        logits = self.fc(self.dropout(combined)).squeeze(-1)\n        # logits: (batch,)\n        \n        return logits\n\n\n# Instantiate\nmodel = LSTMClassifier(\n    vocab_size=vocab_size,\n    embed_dim=cfg.EMBED_DIM,\n    hidden_dim=cfg.HIDDEN_DIM,\n    num_layers=cfg.NUM_LAYERS,\n    dropout=cfg.DROPOUT,\n    bidirectional=cfg.BIDIRECTIONAL,\n    pad_idx=PAD_IDX\n).to(cfg.DEVICE)\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"\\nModel Architecture:\")\nprint(model)\nprint(f\"\\nTotal parameters:     {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T10:28:57.679999Z","iopub.execute_input":"2026-02-13T10:28:57.680403Z","iopub.status.idle":"2026-02-13T10:28:57.706603Z","shell.execute_reply.started":"2026-02-13T10:28:57.680364Z","shell.execute_reply":"2026-02-13T10:28:57.705425Z"}},"outputs":[{"name":"stdout","text":"\n=================================================================\n  STAGE 3: LSTM MODEL DEFINITION\n=================================================================\n\nModel Architecture:\nLSTMClassifier(\n  (embedding): Embedding(652, 128, padding_idx=0)\n  (lstm): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Sequential(\n    (0): Linear(in_features=768, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.3, inplace=False)\n    (3): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\nTotal parameters:     940,033\nTrainable parameters: 940,033\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ============================================================\n#  STAGE 4: TRAINING WITH DEV MONITORING\n# ============================================================\nprint(\"\\n\" + \"=\" * 65)\nprint(\"  STAGE 4: TRAINING\")\nprint(\"=\" * 65)\n\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\noptimizer = torch.optim.Adam(\n    model.parameters(), lr=cfg.LR, weight_decay=cfg.WEIGHT_DECAY\n)\nscheduler = ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.5, patience=3\n)\n\n\ndef evaluate(model, loader, criterion, device):\n    \"\"\"Evaluate model on a dataloader. Returns loss, preds, labels.\"\"\"\n    model.eval()\n    all_preds = []\n    all_labels = []\n    total_loss = 0.0\n    n_batches = 0\n    \n    with torch.no_grad():\n        for batch in loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            lengths = batch[\"length\"].to(device)\n            labels = batch[\"label\"].to(device)\n            \n            logits = model(input_ids, lengths)\n            loss = criterion(logits, labels)\n            \n            total_loss += loss.item()\n            n_batches += 1\n            \n            preds = (torch.sigmoid(logits) >= 0.5).long().cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels.cpu().numpy().astype(int))\n    \n    avg_loss = total_loss / max(n_batches, 1)\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    \n    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    acc = accuracy_score(all_labels, all_preds)\n    \n    return avg_loss, f1, acc, all_preds, all_labels\n\n\n# --- Training Loop ---\nbest_dev_f1 = 0.0\npatience_counter = 0\nhistory = {\"train_loss\": [], \"dev_loss\": [], \"dev_f1\": [], \"dev_acc\": [], \"lr\": []}\n\nprint(f\"\\nStarting training for {cfg.EPOCHS} epochs ...\")\nprint(f\"{'Epoch':>5} | {'Train Loss':>10} | {'Dev Loss':>10} | \"\n      f\"{'Dev F1':>8} | {'Dev Acc':>8} | {'LR':>10} | {'Status'}\")\nprint(\"-\" * 85)\n\nfor epoch in range(1, cfg.EPOCHS + 1):\n    # --- Train ---\n    model.train()\n    train_loss_sum = 0.0\n    n_train_batches = 0\n    \n    for batch in train_loader:\n        input_ids = batch[\"input_ids\"].to(cfg.DEVICE)\n        lengths = batch[\"length\"].to(cfg.DEVICE)\n        labels = batch[\"label\"].to(cfg.DEVICE)\n        \n        optimizer.zero_grad()\n        logits = model(input_ids, lengths)\n        loss = criterion(logits, labels)\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        train_loss_sum += loss.item()\n        n_train_batches += 1\n    \n    avg_train_loss = train_loss_sum / max(n_train_batches, 1)\n    \n    # --- Evaluate on dev (= train, for debugging) ---\n    dev_loss, dev_f1, dev_acc, dev_preds, dev_labels = evaluate(\n        model, dev_loader, criterion, cfg.DEVICE\n    )\n    \n    # --- LR scheduler ---\n    current_lr = optimizer.param_groups[0][\"lr\"]\n    scheduler.step(dev_f1)\n    \n    # --- Track history ---\n    history[\"train_loss\"].append(avg_train_loss)\n    history[\"dev_loss\"].append(dev_loss)\n    history[\"dev_f1\"].append(dev_f1)\n    history[\"dev_acc\"].append(dev_acc)\n    history[\"lr\"].append(current_lr)\n    \n    # --- Early stopping & checkpointing ---\n    status = \"\"\n    if dev_f1 > best_dev_f1:\n        best_dev_f1 = dev_f1\n        patience_counter = 0\n        # Save best model\n        torch.save(model.state_dict(), os.path.join(cfg.SAVE_DIR, \"best_lstm.pt\"))\n        status = \"★ BEST\"\n    else:\n        patience_counter += 1\n        if patience_counter >= cfg.PATIENCE:\n            status = \" STOP\"\n        \n    print(f\"{epoch:>5} | {avg_train_loss:>10.4f} | {dev_loss:>10.4f} | \"\n          f\"{dev_f1:>8.4f} | {dev_acc:>8.4f} | {current_lr:>10.6f} | {status}\")\n    \n    if patience_counter >= cfg.PATIENCE:\n        print(f\"\\n  Early stopping triggered at epoch {epoch} (patience={cfg.PATIENCE})\")\n        break\n\nprint(f\"\\n  Best Dev F1-Macro: {best_dev_f1:.4f}\")\n\n# --- Dev set detailed report (at best checkpoint) ---\nprint(\"\\n--- Dev Set Report (Train-as-Dev, Best Checkpoint) ---\")\nmodel.load_state_dict(torch.load(os.path.join(cfg.SAVE_DIR, \"best_lstm.pt\")))\n\ndev_loss, dev_f1, dev_acc, dev_preds, dev_labels = evaluate(\n    model, dev_loader, criterion, cfg.DEVICE\n)\n\nprint(f\"\\nDev F1-Macro:  {dev_f1:.4f}\")\nprint(f\"Dev Accuracy:  {dev_acc:.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(\n    dev_labels, dev_preds,\n    target_names=[\"Not Ambiguous\", \"Ambiguous\"],\n    digits=4\n))\n\ncm = confusion_matrix(dev_labels, dev_preds)\nprint(f\"Confusion Matrix:\")\nprint(f\"                  Pred_NotAmb  Pred_Amb\")\nprint(f\"  True_NotAmb     {cm[0][0]:>8}     {cm[0][1]:>8}\")\nprint(f\"  True_Amb        {cm[1][0]:>8}     {cm[1][1]:>8}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T10:31:20.724538Z","iopub.execute_input":"2026-02-13T10:31:20.724910Z","iopub.status.idle":"2026-02-13T10:31:49.561269Z","shell.execute_reply.started":"2026-02-13T10:31:20.724867Z","shell.execute_reply":"2026-02-13T10:31:49.559766Z"}},"outputs":[{"name":"stdout","text":"\n=================================================================\n  STAGE 4: TRAINING\n=================================================================\n\nStarting training for 10 epochs ...\nEpoch | Train Loss |   Dev Loss |   Dev F1 |  Dev Acc |         LR | Status\n-------------------------------------------------------------------------------------\n    1 |     0.5566 |     0.4265 |   0.8229 |   0.8256 |   0.001000 | ★ BEST\n    2 |     0.3643 |     0.2443 |   0.8977 |   0.8978 |   0.001000 | ★ BEST\n    3 |     0.2690 |     0.1780 |   0.9378 |   0.9378 |   0.001000 | ★ BEST\n    4 |     0.2146 |     0.1137 |   0.9600 |   0.9600 |   0.001000 | ★ BEST\n    5 |     0.1845 |     0.0846 |   0.9744 |   0.9744 |   0.001000 | ★ BEST\n    6 |     0.1314 |     0.0571 |   0.9867 |   0.9867 |   0.001000 | ★ BEST\n    7 |     0.0950 |     0.0328 |   0.9878 |   0.9878 |   0.001000 | ★ BEST\n    8 |     0.0921 |     0.0221 |   0.9922 |   0.9922 |   0.001000 | ★ BEST\n    9 |     0.0709 |     0.0165 |   0.9956 |   0.9956 |   0.001000 | ★ BEST\n   10 |     0.0717 |     0.0161 |   0.9944 |   0.9944 |   0.001000 | \n\n  Best Dev F1-Macro: 0.9956\n\n--- Dev Set Report (Train-as-Dev, Best Checkpoint) ---\n\nDev F1-Macro:  0.9956\nDev Accuracy:  0.9956\n\nClassification Report:\n               precision    recall  f1-score   support\n\nNot Ambiguous     0.9934    0.9978    0.9956       450\n    Ambiguous     0.9978    0.9933    0.9955       450\n\n     accuracy                         0.9956       900\n    macro avg     0.9956    0.9956    0.9956       900\n weighted avg     0.9956    0.9956    0.9956       900\n\nConfusion Matrix:\n                  Pred_NotAmb  Pred_Amb\n  True_NotAmb          449            1\n  True_Amb               3          447\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================================================\n#  STAGE 5: FINAL TEST EVALUATION & SAVE\n# ============================================================\nprint(\"\\n\" + \"=\" * 65)\nprint(\"  STAGE 5: FINAL TEST EVALUATION & MODEL SAVING\")\nprint(\"=\" * 65)\n\n# --- Test evaluation ---\ntest_loss, test_f1, test_acc, test_preds, test_labels_arr = evaluate(\n    model, test_loader, criterion, cfg.DEVICE\n)\n\nprint(f\"\\n{'='*40}\")\nprint(f\"  FINAL TEST RESULTS\")\nprint(f\"{'='*40}\")\n\nif has_test_labels:\n    print(f\"  Test F1-Macro:  {test_f1:.4f}\")\n    print(f\"  Test Accuracy:  {test_acc:.4f}\")\n    print(f\"\\nClassification Report:\")\n    print(classification_report(\n        test_labels_arr, test_preds,\n        target_names=[\"Not Ambiguous\", \"Ambiguous\"],\n        digits=4\n    ))\n    \n    cm_test = confusion_matrix(test_labels_arr, test_preds)\n    print(f\"Confusion Matrix:\")\n    print(f\"                  Pred_NotAmb  Pred_Amb\")\n    print(f\"  True_NotAmb     {cm_test[0][0]:>8}     {cm_test[0][1]:>8}\")\n    print(f\"  True_Amb        {cm_test[1][0]:>8}     {cm_test[1][1]:>8}\")\nelse:\n    print(f\"  No test labels available — predictions saved only.\")\n    print(f\"  Prediction distribution:\")\n    print(f\"    Not Ambiguous (0): {(test_preds == 0).sum()}\")\n    print(f\"    Ambiguous (1):     {(test_preds == 1).sum()}\")\n\n# --- Save predictions ---\ndf_test[\"predicted_label\"] = test_preds\n\noutput_cols = [\"id\", \"question\", \"norm_text\", \"predicted_label\"]\nif has_test_labels:\n    output_cols.insert(3, \"is_ambiguous\")\ndf_test[output_cols].to_csv(\n    os.path.join(cfg.SAVE_DIR, \"lstm_test_predictions.csv\"),\n    index=False, encoding=\"utf-8-sig\"\n)\n\n# Submission file\nsubmission = df_test[[\"id\", \"predicted_label\"]].copy()\nsubmission.columns = [\"id\", \"is_ambiguous\"]\nsubmission.to_csv(os.path.join(cfg.SAVE_DIR, \"submission.csv\"), index=False)\n\n# --- Save model artifacts ---\n# 1. Model weights (already saved as best_lstm.pt)\nprint(f\"\\n--- Saving Model Artifacts ---\")\n\n# 2. Vocabulary\nvocab_path = os.path.join(cfg.SAVE_DIR, \"vocab.pkl\")\nwith open(vocab_path, \"wb\") as f:\n    pickle.dump({\n        \"word2idx\": word2idx,\n        \"idx2word\": idx2word,\n        \"vocab_size\": vocab_size\n    }, f)\nprint(f\"   Vocabulary saved: {vocab_path}\")\n\n# 3. Config\nconfig_dict = {\n    \"max_len\": cfg.MAX_LEN,\n    \"embed_dim\": cfg.EMBED_DIM,\n    \"hidden_dim\": cfg.HIDDEN_DIM,\n    \"num_layers\": cfg.NUM_LAYERS,\n    \"dropout\": cfg.DROPOUT,\n    \"bidirectional\": cfg.BIDIRECTIONAL,\n    \"vocab_size\": vocab_size,\n    \"pad_idx\": PAD_IDX,\n    \"best_dev_f1\": best_dev_f1,\n}\nif has_test_labels:\n    config_dict[\"test_f1\"] = test_f1\n    config_dict[\"test_acc\"] = test_acc\n\nconfig_path = os.path.join(cfg.SAVE_DIR, \"config.pkl\")\nwith open(config_path, \"wb\") as f:\n    pickle.dump(config_dict, f)\nprint(f\"   Config saved: {config_path}\")\n\n# 4. Training history\nhistory_path = os.path.join(cfg.SAVE_DIR, \"training_history.pkl\")\nwith open(history_path, \"wb\") as f:\n    pickle.dump(history, f)\nprint(f\"   Training history saved: {history_path}\")\n\nprint(f\"   Model weights: {os.path.join(cfg.SAVE_DIR, 'best_lstm.pt')}\")\nprint(f\"   Predictions: lstm_test_predictions.csv\")\nprint(f\"   Submission: submission.csv\")\n\n# --- Summary ---\nprint(\"\\n\" + \"=\" * 65)\nprint(\"  PIPELINE COMPLETE — SUMMARY\")\nprint(\"=\" * 65)\nprint(f\"  Model:          BiLSTM (2-layer, hidden={cfg.HIDDEN_DIM})\")\nprint(f\"  Vocab size:     {vocab_size:,}\")\nprint(f\"  Max seq length: {cfg.MAX_LEN}\")\nprint(f\"  Best Dev F1:    {best_dev_f1:.4f}\")\nif has_test_labels:\n    print(f\"  Test F1-Macro:  {test_f1:.4f}\")\n    print(f\"  Test Accuracy:  {test_acc:.4f}\")\nprint(f\"  Saved files:    best_lstm.pt, vocab.pkl, config.pkl, \"\n      f\"training_history.pkl, submission.csv\")\nprint(\"=\" * 65)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T10:34:11.529322Z","iopub.execute_input":"2026-02-13T10:34:11.529717Z","iopub.status.idle":"2026-02-13T10:34:11.652564Z","shell.execute_reply.started":"2026-02-13T10:34:11.529682Z","shell.execute_reply":"2026-02-13T10:34:11.651246Z"}},"outputs":[{"name":"stdout","text":"\n=================================================================\n  STAGE 5: FINAL TEST EVALUATION & MODEL SAVING\n=================================================================\n\n========================================\n  FINAL TEST RESULTS\n========================================\n  Test F1-Macro:  0.9000\n  Test Accuracy:  0.9000\n\nClassification Report:\n               precision    recall  f1-score   support\n\nNot Ambiguous     0.9000    0.9000    0.9000        50\n    Ambiguous     0.9000    0.9000    0.9000        50\n\n     accuracy                         0.9000       100\n    macro avg     0.9000    0.9000    0.9000       100\n weighted avg     0.9000    0.9000    0.9000       100\n\nConfusion Matrix:\n                  Pred_NotAmb  Pred_Amb\n  True_NotAmb           45            5\n  True_Amb               5           45\n\n--- Saving Model Artifacts ---\n   Vocabulary saved: /kaggle/working/vocab.pkl\n   Config saved: /kaggle/working/config.pkl\n   Training history saved: /kaggle/working/training_history.pkl\n   Model weights: /kaggle/working/best_lstm.pt\n   Predictions: lstm_test_predictions.csv\n   Submission: submission.csv\n\n=================================================================\n  PIPELINE COMPLETE — SUMMARY\n=================================================================\n  Model:          BiLSTM (2-layer, hidden=128)\n  Vocab size:     652\n  Max seq length: 64\n  Best Dev F1:    0.9956\n  Test F1-Macro:  0.9000\n  Test Accuracy:  0.9000\n  Saved files:    best_lstm.pt, vocab.pkl, config.pkl, training_history.pkl, submission.csv\n=================================================================\n","output_type":"stream"}],"execution_count":12}]}