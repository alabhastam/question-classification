{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14821512,"sourceType":"datasetVersion","datasetId":9478698}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-13T09:59:42.299359Z","iopub.execute_input":"2026-02-13T09:59:42.300301Z","iopub.status.idle":"2026-02-13T09:59:42.706013Z","shell.execute_reply.started":"2026-02-13T09:59:42.300265Z","shell.execute_reply":"2026-02-13T09:59:42.704803Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/datasets/aabdollahii/university-questions/train.json\n/kaggle/input/datasets/aabdollahii/university-questions/test.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n#  FULL PIPELINE: LSTM for Persian Question Ambiguity Detection\n# ============================================================\n#  Stage 1: Preprocessing V2 (train + test)\n#  Stage 2: Vocabulary & Dataset\n#  Stage 3: LSTM Model Definition\n#  Stage 4: Training with dev (train-as-dev) monitoring\n#  Stage 5: Final test evaluation & model saving\n# ============================================================\n\nimport json\nimport re\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix,\n    f1_score, accuracy_score\n)\n\nfrom hazm import Normalizer, word_tokenize\n\n# ============================================================\n#  CONFIG\n# ============================================================\nclass Config:\n    # Paths\n    TRAIN_PATH = \"/kaggle/input/datasets/aabdollahii/university-questions/train.json\"\n    TEST_PATH = \"/kaggle/input/datasets/aabdollahii/university-questions/test.json\"\n    SAVE_DIR = \"/kaggle/working/\"\n    \n    # Preprocessing\n    MAX_LEN = 64           # max tokens per question (will verify from data)\n    MIN_FREQ = 2           # min word frequency to include in vocab\n    \n    # Model\n    EMBED_DIM = 128\n    HIDDEN_DIM = 128\n    NUM_LAYERS = 2\n    DROPOUT = 0.3\n    BIDIRECTIONAL = True\n    \n    # Training\n    BATCH_SIZE = 32\n    EPOCHS = 30\n    LR = 1e-3\n    WEIGHT_DECAY = 1e-5\n    PATIENCE = 7           # early stopping patience\n    \n    # Device\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Reproducibility\n    SEED = 42\n\ncfg = Config()\n\n# Set seeds\ntorch.manual_seed(cfg.SEED)\nnp.random.seed(cfg.SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(cfg.SEED)\n\nprint(f\"Device: {cfg.DEVICE}\")\nprint(f\"PyTorch version: {torch.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T10:07:15.727745Z","iopub.execute_input":"2026-02-13T10:07:15.728133Z","iopub.status.idle":"2026-02-13T10:07:15.741395Z","shell.execute_reply.started":"2026-02-13T10:07:15.728081Z","shell.execute_reply":"2026-02-13T10:07:15.740059Z"}},"outputs":[{"name":"stdout","text":"Device: cpu\nPyTorch version: 2.8.0+cu126\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================\n#  STAGE 1: PREPROCESSING V2\n# ============================================================\nprint(\"\\n\" + \"=\" * 65)\nprint(\"  STAGE 1: PREPROCESSING V2\")\nprint(\"=\" * 65)\n\nformal_normalizer = Normalizer()\n\ndef normalize_v2(text):\n    \"\"\"\n    V2 normalization pipeline — safe version (no InformalNormalizer).\n    1. Hazm formal normalization (handles ی/ک, spacing, etc.)\n    2. Arabic char normalization\n    3. Clean punctuation/extra whitespace\n    \"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return \"\"\n    \n    # Step 1: Hazm formal normalization\n    text = formal_normalizer.normalize(text)\n    \n    # Step 2: Additional Arabic → Persian char normalization\n    text = text.replace(\"ي\", \"ی\").replace(\"ك\", \"ک\")\n    text = text.replace(\"ؤ\", \"و\").replace(\"إ\", \"ا\").replace(\"أ\", \"ا\")\n    text = text.replace(\"ة\", \"ه\")\n    \n    # Step 3: Normalize various dashes and special chars\n    text = re.sub(r'[ـ]+', '', text)              # remove kashida (tatweel)\n    text = re.sub(r'[‌]+', ' ', text)              # replace ZWNJ with space (hazm handles most)\n    \n    # Step 4: Keep Persian/Arabic letters, digits, basic punctuation, spaces\n    text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\uFB50-\\uFDFF\\uFE70-\\uFEFF'\n                  r'a-zA-Z0-9۰-۹٠-٩\\s\\.\\?\\!،؛]', ' ', text)\n    \n    # Step 5: Clean extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n\ndef tokenize_text(text):\n    \"\"\"Tokenize using Hazm word_tokenize after normalization.\"\"\"\n    if not text:\n        return []\n    return word_tokenize(text)\n\n\n# --- Load Data ---\nprint(\"Loading train.json ...\")\nwith open(cfg.TRAIN_PATH, \"r\", encoding=\"utf-8\") as f:\n    train_data = json.load(f)\ndf_train = pd.DataFrame(train_data)\nprint(f\"  Train shape: {df_train.shape}\")\nprint(f\"  Label distribution:\\n{df_train['is_ambiguous'].value_counts().to_string()}\")\n\nprint(\"\\nLoading test.json ...\")\nwith open(cfg.TEST_PATH, \"r\", encoding=\"utf-8\") as f:\n    test_data = json.load(f)\ndf_test = pd.DataFrame(test_data)\nprint(f\"  Test shape: {df_test.shape}\")\nhas_test_labels = \"is_ambiguous\" in df_test.columns\nif has_test_labels:\n    print(f\"  Test label distribution:\\n{df_test['is_ambiguous'].value_counts().to_string()}\")\n\n# --- Apply Normalization ---\nprint(\"\\nNormalizing train questions ...\")\ndf_train[\"norm_text\"] = df_train[\"question\"].apply(normalize_v2)\nprint(\"Normalizing test questions ...\")\ndf_test[\"norm_text\"] = df_test[\"question\"].apply(normalize_v2)\n\n# --- Tokenize ---\nprint(\"Tokenizing train ...\")\ndf_train[\"tokens\"] = df_train[\"norm_text\"].apply(tokenize_text)\nprint(\"Tokenizing test ...\")\ndf_test[\"tokens\"] = df_test[\"norm_text\"].apply(tokenize_text)\n\n# --- Show Samples ---\nprint(\"\\n--- Train Samples ---\")\nfor i in range(5):\n    print(f\"  [{df_train['is_ambiguous'].iloc[i]}] {df_train['question'].iloc[i]}\")\n    print(f\"       → {df_train['tokens'].iloc[i][:15]} ...\")\n    print()\n\n# --- Sequence Length Analysis ---\ntrain_lengths = df_train[\"tokens\"].apply(len)\nprint(f\"Token length stats (train):\")\nprint(f\"  Mean:   {train_lengths.mean():.1f}\")\nprint(f\"  Median: {train_lengths.median():.1f}\")\nprint(f\"  95th %: {train_lengths.quantile(0.95):.0f}\")\nprint(f\"  99th %: {train_lengths.quantile(0.99):.0f}\")\nprint(f\"  Max:    {train_lengths.max()}\")\n\n# Update MAX_LEN based on data (cover 95th percentile)\nsuggested_max_len = int(train_lengths.quantile(0.95)) + 2\nif suggested_max_len != cfg.MAX_LEN:\n    print(f\"\\n   Updating MAX_LEN: {cfg.MAX_LEN} → {suggested_max_len}\")\n    cfg.MAX_LEN = suggested_max_len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T10:07:26.129209Z","iopub.execute_input":"2026-02-13T10:07:26.129555Z"}},"outputs":[{"name":"stdout","text":"\n=================================================================\n  STAGE 1: PREPROCESSING V2\n=================================================================\nLoading train.json ...\n  Train shape: (900, 4)\n  Label distribution:\nis_ambiguous\n0    450\n1    450\n\nLoading test.json ...\n  Test shape: (100, 4)\n  Test label distribution:\nis_ambiguous\n0    50\n1    50\n\nNormalizing train questions ...\nNormalizing test questions ...\nTokenizing train ...\n","output_type":"stream"}],"execution_count":null}]}